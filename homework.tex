\documentclass[twoside,11pt]{homework}

\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{graphicx}

\coursename{COMS 4771 Machine Learning (Spring 2020)} 

\studname{Angela Wang}     
\studmail{aw3062@columbia.edu}
\collab{Jane Pan, Stan Liao}
\hwNo{1}      
\date{02/21/2020}  
\begin{document}
\maketitle

\section*{Problem 1} 
\subsection*{(i)}
	The likelihood function of $\theta$ is 
	\begin{align*}
		\mathcal{L}(\theta|X):=& \mathbb{P}(X|\theta) \\
		=& \prod_{i=1}^n \mathbb{P}(x_i | \theta) \tag{i.i.d.}
	\end{align*}
	where $\mathbb{P}(x_i | \theta) = \frac{1}{b-a}$ if $x\in [a,b]$, and $\mathbb{P}(x_i | \theta) = 0$ otherwise. 
	The maximum likelihood estimate is the $\theta$ that maximizes $\mathcal{L}(\theta|X)$, given by
	\begin{align*}
		 \text{arg}\max_\theta \mathcal{L}(\theta|X) 
		=  \text{arg}\max_\theta \log \mathcal{L}(\theta|X) 
		= \text{arg}\max_\theta \log \prod_{i=1}^n \mathbb{P}(x_i | \theta)
	\end{align*}
	To maximize the likelihood, we choose $a\leq \min(x_1,\dots,x_n)$, and  $b\geq \max(x_1,\dots,x_n)$,
	\begin{align*}
		& \text{arg}\max_\theta \mathcal{L}(\theta|X) \\
		= & \text{arg}\max_\theta \log \left(\frac{1}{b-a}\right)^n \\
		= & \text{arg} \max_\theta \, n \log\left(\frac{1}{b-a}\right)
	\end{align*}
	To maximize the logarithm, we should minimize $b-a$. Hence $\theta_{\text{ML}} = (a_{\text{ML}} ,b_{\text{ML}} )$, where
	\begin{align*}
		a_{\text{ML}}  &= \min(x_1,\dots,x_n) \\
		b_{\text{ML}}  &= \max(x_1,\dots,x_n)
	\end{align*}
\subsection*{(ii)}
	\begin{proof}
		Define $\eta = g(\theta)$. Suppose $g$ is bijective, then $\exists g^{-1}$ such that 
		\begin{align*}
			\mathcal{L}(\theta|X) = \mathcal{L}( g^{-1}(\eta) |X)
		\end{align*}
		whose maximum value is $\mathcal{L}(\theta_{\text{ML}}|X) $. 
		To maximize $\mathcal{L}( g^{-1}(\eta) |X)$, choose $\eta_{\text{ML}}$
		such that $g^{-1}(\eta_{\text{ML}})=\theta_{\text{ML}}$. We have $\eta_{\text{ML}}=g(\theta_{\text{ML}})$.\\
		\begin{remark} 
			For the case when $g$ is not bijective, we apply the Invertible Function Theorem to find local inverses for $g$.		
		\end{remark}
	\end{proof}
\subsection*{(iii)}
	Let $X_1,\dots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ be i.i.d random variables.
\subsubsection*{(a)}
	\begin{claim}
		$\frac{1}{n} \sum_{i=1}^n X_i $ is a consistent and unbiased estimator for $\mu$.
	\end{claim}
	\begin{proof}
		By Law of Large Numbers, 
		$ \lim\limits_{n\rightarrow \infty} \frac{1}{n} \sum_{i=1}^n X_i = \mu$.\\
		Since $\mathbb{E}[\frac{1}{n} \sum_{i=1}^n X_i]
		=\frac{1}{n} \sum_{i=1}^n \mathbb{E}[X_i]
		=\frac{1}{n} \sum_{i=1}^n \mu
		=\mu
		$, the estimator is unbiased.
	\end{proof}
	\begin{claim}
		$\frac{1}{n} \sum_{i=1}^n X_i + \frac{1}{n} X_1- \frac{1}{n} X_2$ is a consistent and unbiased estimator for $\mu$.
	\end{claim}
\subsubsection*{(b)}	
	\begin{claim}
		$ \frac{1}{n} \sum_{i=1}^n X_i +\frac{1}{n}$ is a biased but consistent estimator for $\mu$.
	\end{claim}
	\begin{proof}
		$ \lim\limits_{n\rightarrow \infty} (\frac{1}{n} \sum_{i=1}^n X_i +\frac{1}{n})= \mu$.
		$\mathbb{E}[\frac{1}{n} \sum_{i=1}^n X_i +\frac{1}{n}]
		=\frac{1}{n} \sum_{i=1}^n \mathbb{E}[X_i] +\frac{1}{n}
		=\mu+\frac{1}{n}.
		$
	\end{proof}
	\begin{claim}
		$ \frac{1}{n} \sum_{i=1}^n X_i +\frac{1}{n^2}$ is a biased but consistent estimator for $\mu$.
	\end{claim}
\subsubsection*{(c)}
	\begin{claim}
		$X_1$ is an unbiased but inconsistent estimator for $\mu$.
	\end{claim}
	\begin{proof}
		$X_1$ is not consistent since its distribution does not become more concentrated around $\mu$ 
		as the sample size increases.
		$\mathbb{E}[X_1] =\mu$ so $X_1$ is an unbiased estimator.
	\end{proof}
	\begin{claim}
		$X_2$ is an unbiased but inconsistent estimator for $\mu$.
	\end{claim}
\subsubsection*{(d)}
	\begin{claim}
		The constant $1$ is a biased and inconsistent estimator for $\mu$.
	\end{claim}
	\begin{claim}
		$1+X_1$ is a biased and inconsistent estimator for $\mu$.
	\end{claim}
	
\section*{Problem 2} 
\subsection*{(i)}
	Let $\Pi$ be the profit. We can write
	\begin{align*}
		\Pi &= \mathds{1}_{D\geq Q} (P-C)Q + \mathds{1}_{D\leq Q} \left[(P-C)D-C(Q-D)\right] \\
		&= -CQ +\mathds{1}_{D\geq Q} PQ+\mathds{1}_{D\leq Q} PD
	\end{align*}
	The expected profit is given by
	\begin{align*}
		\mathbb{E} [\Pi] &= -CQ+\int_Q^\infty PQ\mathbb{P}(D)dD
		+ \int_{-\infty}^Q PD\mathbb{P}(D)dD \\
		&= -CQ+ PQ\int_Q^\infty \mathbb{P}(D)dD
		+ P\int_{-\infty}^Q D\mathbb{P}(D)dD
	\end{align*}
\subsection*{(ii)}
	\begin{proof}
	Taking the derivative,
	\begin{align*}
		\frac{d\mathbb{E} [\Pi]}{dQ} &= -C + P\int_Q^\infty \mathbb{P}(D)dD
		+ PQ \frac{d}{dQ} \int_Q^\infty \mathbb{P}(D)dD
		+ P \frac{d}{dQ}  \int_{-\infty}^Q D\mathbb{P}(D)dD \\
		&= -C + P\int_Q^\infty \mathbb{P}(D)dD
		- PQ \mathbb{P}(Q)
		+ P Q\mathbb{P}(Q) \tag{Fundamental Theorem of Calculus}\\
		&=-C+P\int_Q^\infty \mathbb{P}(D)dD\\
		&=-C +P(1-F(Q))
	\end{align*}
	To maximize profit, we let $\frac{d\mathbb{E} [\Pi]}{dQ} =-C +P(1-F(Q^*)= 0$. Rearranging, we have
	\begin{align*}
		Q^*= F^{-1}\left(1-\frac{C}{P}\right)
	\end{align*}
	\end{proof}

\section*{Problem 3} 
\subsection*{(i)}
	The error rate is the probability of getting false positive or false negatives:
	\begin{align*}
		& \mathbb{P}[f_t(x)\neq y] \\
		=& \mathbb{P}[Y=y_1 | X\leq t] + \mathbb{P}[Y=y_2 | X\geq t] \\
		=& \int_{-\infty}^t  \mathbb{P}[Y=y_1 | X=x] dx + \int_{t}^\infty  \mathbb{P}[Y=y_2 | X=x] dx
	\end{align*}
\subsection*{(ii)}
	Differentiating the error rate, we have
	\begin{align*}
		& \frac{d}{dt} \mathbb{P}[f_t(x)\neq y]  \\
		=& \, \frac{d}{dt}\int_{-\infty}^t  \mathbb{P}[Y=y_1 | X=x] dx - \frac{d}{dt} \int_{\infty}^t  \mathbb{P}[Y=y_2 | X=x] dx \\
		=&\,  \mathbb{P}[Y=y_1 | X=t] - \mathbb{P}[Y=y_2 | X=t] \tag{Foundamental Theorem of Calculus}
	\end{align*}
	To minimize the error rate, we set $\frac{d}{dt} \mathbb{P}[f_t(x)\neq y] = 0$,
	\begin{align*}
		\mathbb{P}[Y=y_1 | X=t] &= \mathbb{P}[Y=y_2 | X=t] \\
		\mathbb{P}[ X=t | Y=y_1 ] \frac{ \mathbb{P}[ Y=y_1 ]}{ \mathbb{P}[ X=t]}
		&= \mathbb{P}[ X=t | Y=y_2 ] \frac{ \mathbb{P}[ Y=y_2 ]}{ \mathbb{P}[ X=t]} \tag{Bayes Rule}\\
		\mathbb{P}[ X=t | Y=y_1 ] \mathbb{P}[ Y=y_1 ] 
		&= \mathbb{P}[ X=t | Y=y_2 ] \mathbb{P}[ Y=y_2 ]
	\end{align*}
\subsection*{(iii)}
	Let the distribution of $\mathbb{P}[X|Y=y_1]$ be $\mathcal{N}(\mu_1,\sigma_1^2)$, 
	and the distribution of $\mathbb{P}[X|Y=y_2]$ be $\mathcal{N}(\mu_2,\sigma_2^2)$.
	By Bayesian Decision Theory, the Bayes error is the overlapped area under $\mathcal{N}(\mu_1,\sigma_1^2)$
	and $\mathcal{N}(\mu_2,\sigma_2^2)$. \\
	Example in which $f_t$ achieves Bayesian error rate:
	\begin{align*}
		\mathbb{P}[X|Y=y_1] &\sim \mathcal{N}(-1, 1)\\
		\mathbb{P}[X|Y=y_2] &\sim \mathcal{N}(1, 1) \\
		t&=0
	\end{align*}
	Example in which $\forall t\in \mathbb{R}$, $f_t$ does not achieve Bayesian error rate:
	\begin{align*}
		\mathbb{P}[X|Y=y_1] &\sim \mathcal{N}(0, 1)\\
		\mathbb{P}[X|Y=y_2] &\sim \mathcal{N}(0, 2) \\
	\end{align*}


\section*{Problem 4} 
\subsection*{(i)}
	\begin{proof}
		Since $M^{T} = (A^{T}A)^{T} = A^{T}A = M$, M is symmetric. 
		For any column vector $z\in \mathbb{R}^d$, $z^TMz = z^TA^TAz = (Az)^T (Az) = (Az)\cdot(Az) \geq 0$, 
		so M is positive semi-definite.
	\end{proof}
\subsection*{(ii)}
	\begin{proof}
		Base case: if $N=1$, then by definition, 
		\begin{align*}
			\beta^{(1)} &= \beta^{(0)}+ \eta  A^{T} (b-A \beta^{(0)}) \\
			&= \eta v - \eta M \beta^{(0)} \\
			&= \eta v 
		\end{align*}
		Assuming the statement $\beta^{(N)} = \eta \sum_{k=0}^{N-1} (I-\eta M)^k v$ is true for $N=n$, then for $N=n+1$ we have
		\begin{align*}
			\beta^{(n+1)} &= (I-\eta M) \beta^{(n)} + \eta v \tag{definition} \\
			&= (I-\eta M) \eta  \sum_{k=0}^{n-1} (I-\eta M)^k v + \eta v \tag{induction hypothesis} \\
			&= \eta  \sum_{k=0}^{n-1} (I-\eta M)^{(k+1)} v + \eta (I-\eta M)^0 v \\
			&= \eta  \sum_{k=1}^{n} (I-\eta M)^{(k)} v + \eta (I-\eta M)^0 v \\
			&= \eta  \sum_{k=0}^{n} (I-\eta M)^{(k)} v
		\end{align*}
	\end{proof}
\subsection*{(iii)}
	Since $M$ is a real symmetric matrix, it has a decomposition $M = P D P^{-1}$, 
	where $P$ is a matrix composed of orthogonal eigenvectors corresponding to distinct eigenvalues,
	and $D:= \text{diag}(\lambda_1,\dots, \lambda_d)$.
	\begin{align*}
		&\eta \sum_{k=0}^{N-1} (I-\eta M)^k \\
		=& \eta \sum_{k=0}^{N-1} \sum_{i=0}^{k} I^{k-i}(-\eta M)^i \\
		=&  \sum_{k=0}^{N-1} \sum_{i=0}^{k} (-1)^i\eta^{i+1} M^{i} \\
		=&  \sum_{k=0}^{N-1} \sum_{i=0}^{k} (-1)^i\eta^{i+1} PD^{i}P^{-1} \\
		=& P  \left(\sum_{k=0}^{N-1} \sum_{i=0}^{k}  (-1)^i\eta^{i+1}  \text{diag}(\lambda_1^i,\dots, \lambda_d^i) \right) P^{-1} \\
		=& P \, \text{diag} \left(\sum_{k=0}^{N-1} \sum_{i=0}^{k}  (-1)^i\eta^{i+1} \lambda_1^i,\dots, 
		\sum_{k=0}^{N-1} \sum_{i=0}^{k}  (-1)^i\eta^{i+1} \lambda_d^i \right)  P^{-1}\\
		=& P \, \text{diag} \left(
			\sum_{i=0}^{N-1} (N-i) (-1)^{i}\eta^{i+1}\lambda^i_1
			\dots, 
			\sum_{i=0}^{N-1} (N-i) (-1)^{i}\eta^{i+1}\lambda^i_d
		\right)  P^{-1}
	\end{align*}
	Since $I$, $M$ are symmetric, $\eta \sum_{k=0}^{N-1} (I-\eta M)^k$ is symmetric, 
	and its eiganvalues $\lambda_1',\dots, \lambda_d'$ are given by
	\begin{align*}
		\lambda_j' = \sum_{i=0}^{N-1} (N-i) (-1)^{i}\eta^{i+1}\lambda^i_j
	\end{align*}
	for any $j=1,\dots,d$.
\subsection*{(iv)}
	\begin{proof}
	We start by deriving a iterative formula for the difference
	\begin{align*}
		& ||\beta^{(N)} - \hat{\beta}||_2^2 \\
		=& \left( (I-\eta M)^N \hat\beta \right)^T \left( (I-\eta M)^N \hat\beta \right) \\
		=& \, \hat\beta^T (I-\eta M)^{2N} \hat\beta \\
		\leq& \, \hat\beta^T (I-2N\eta M) \hat\beta \tag{Taylor's Theorem} \\
		\leq& \, || I-2N\eta M ||_2 \cdot ||\hat{\beta}||_2^2 \\
		\leq & \, (1-2N\eta \lambda_{\text{min}}) ||\hat{\beta}||_2^2 
		\tag{Since $\sigma_{\text{max}}(I-2N\eta M) = 1-2N\eta \sigma_{\text{min}}(M)$} \\
		\leq& \, e^{-2N\eta\lambda_{\text{min}}}||\hat{\beta}||_2^2 
	\end{align*}
	\end{proof}




\section*{Problem 5}
\subsection*{Part 0}
	Ignoring the sensitive attribute does not guarantee fairness because there may be 
	other attributes with correlation to that sensitive attributes. 
	For instance, assume that thhe sensitive attribute is income level, 
	but you also have attributes like number of cars owned, average spending per month, etc. 
	Simply removing income level does not prevent the model from maintaining a bias since it will still use 
	number of cars owned, average spending per month, etc. to classify the data. As a result, 
	the model will likely fail to maintain fairness across the sensitive distribution.
\subsection*{Part 1}  
	$$P[Y=1]=P_a[Y=1]$$ 
	$$P_a[Y=1]=P[Y=1|a=0]=P[Y=0|a=1]$$
	$$P[Y=1|a=0]=P[Y=0|a=1]$$
	$$P_0[Y=1]=P_1[Y=1]$$
	Generalizing:
	$$P_{a}[Y=1] = P_{a'}[Y=1] \forall a, a ' \in A$$
	$$P[y=1] = P_a[y=1] \forall a \in A$$\\








\end{document} 
