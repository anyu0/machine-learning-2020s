\documentclass[twoside,11pt]{homework}

\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{graphicx}

\coursename{COMS 4771 Machine Learning (Spring 2020)} 

\studname{Angela Wang}     
\studmail{aw3062@columbia.edu}
\collab{Jane Pan, Stan Liao}
\hwNo{1}      
\date{02/21/2020}  
\begin{document}
\maketitle

\section*{Problem 1} 
\subsection*{(i)}
	The likelihood function of $\theta$ is 
	\begin{align*}
		\mathcal{L}(\theta|X):=& \mathbb{P}(X|\theta) \\
		=& \prod_{i=1}^n \mathbb{P}(x_i | \theta) \tag{i.i.d.}
	\end{align*}
	where $\mathbb{P}(x_i | \theta) = \frac{1}{b-a}$ if $x\in [a,b]$, and $\mathbb{P}(x_i | \theta) = 0$ otherwise. 
	The maximum likelihood estimate is the $\theta$ that maximizes $\mathcal{L}(\theta|X)$, given by
	\begin{align*}
		 \text{arg}\max_\theta \mathcal{L}(\theta|X) 
		=  \text{arg}\max_\theta \log \mathcal{L}(\theta|X) 
		= \text{arg}\max_\theta \log \prod_{i=1}^n \mathbb{P}(x_i | \theta)
	\end{align*}
	To maximize the likelihood, we choose $a\leq \min(x_1,\dots,x_n)$, and  $b\geq \max(x_1,\dots,x_n)$,
	\begin{align*}
		& \text{arg}\max_\theta \mathcal{L}(\theta|X) \\
		= & \text{arg}\max_\theta \log \left(\frac{1}{b-a}\right)^n \\
		= & \text{arg} \max_\theta \, n \log\left(\frac{1}{b-a}\right)
	\end{align*}
	To maximize the logarithm, we should minimize $b-a$. Hence $\theta = (a,b)$, where
	\begin{align*}
		a &= \min(x_1,\dots,x_n) \\
		b &= \max(x_1,\dots,x_n)
	\end{align*}
\section*{Problem 2} 
\subsection*{(i)}

\section*{Problem 3} 
\subsection*{(i)}
	The error rate is the probability of getting false positive or false negatives:
	\begin{align*}
		& \mathbb{P}[f_t(x)\neq y] \\
		=& \mathbb{P}[Y=y_1 | X\leq t] + \mathbb{P}[Y=y_2 | X\geq t] \\
		=& \int_{-\infty}^t  \mathbb{P}[Y=y_1 | X=x] dx + \int_{t}^\infty  \mathbb{P}[Y=y_2 | X=x] dx
	\end{align*}
\subsection*{(ii)}
	Differentiating the error rate, we have
	\begin{align*}
		& \frac{d}{dt} \mathbb{P}[f_t(x)\neq y]  \\
		=& \, \frac{d}{dt}\int_{-\infty}^t  \mathbb{P}[Y=y_1 | X=x] dx - \frac{d}{dt} \int_{\infty}^t  \mathbb{P}[Y=y_2 | X=x] dx \\
		=&\,  \mathbb{P}[Y=y_1 | X=t] - \mathbb{P}[Y=y_2 | X=t] \tag{Foundamental Theorem of Calculus}
	\end{align*}
	To minimize the error rate, we set $\frac{d}{dt} \mathbb{P}[f_t(x)\neq y] = 0$,
	\begin{align*}
		\mathbb{P}[Y=y_1 | X=t] &= \mathbb{P}[Y=y_2 | X=t] \\
		\mathbb{P}[ X=t | Y=y_1 ] \frac{ \mathbb{P}[ Y=y_1 ]}{ \mathbb{P}[ X=t]}
		&= \mathbb{P}[ X=t | Y=y_2 ] \frac{ \mathbb{P}[ Y=y_2 ]}{ \mathbb{P}[ X=t]} \tag{Bayes Rule}\\
		\mathbb{P}[ X=t | Y=y_1 ] \mathbb{P}[ Y=y_1 ] 
		&= \mathbb{P}[ X=t | Y=y_2 ] \mathbb{P}[ Y=y_2 ]
	\end{align*}
\subsection*{(iii)}
	Let the distribution of $\mathbb{P}[X|Y=y_1]$ be $\mathcal{N}(\mu_1,\sigma_1^2)$, 
	and the distribution of $\mathbb{P}[X|Y=y_2]$ be $\mathcal{N}(\mu_2,\sigma_2^2)$.
	By Bayesian Decision Theory, the Bayes error is the overlapped area under $\mathcal{N}(\mu_1,\sigma_1^2)$
	and $\mathcal{N}(\mu_2,\sigma_2^2)$. \\
	Example in which $f_t$ achieves Bayesian error rate:
	\begin{align*}
		\mathbb{P}[X|Y=y_1] &\sim \mathcal{N}(-1, 1)\\
		\mathbb{P}[X|Y=y_2] &\sim \mathcal{N}(1, 1) \\
		t&=0
	\end{align*}
	Example in which $\forall t\in \mathbb{R}$, $f_t$ does not achieve Bayesian error rate:
	\begin{align*}
		\mathbb{P}[X|Y=y_1] &\sim \mathcal{N}(0, 1)\\
		\mathbb{P}[X|Y=y_2] &\sim \mathcal{N}(0, 2) \\
	\end{align*}


\section*{Problem 4} 
\subsection*{(i)}
	\begin{proof}
		Since $M^{T} = (A^{T}A)^{T} = A^{T}A = M$, M is symmetric. 
		For any column vector $z\in \mathbb{R}^d$, $z^TMz = z^TA^TAz = (Az)^T (Az) = (Az)\cdot(Az) \geq 0$, 
		so M is positive semi-definite.
	\end{proof}
\subsection*{(ii)}
	\begin{proof}
		Base case: if $N=1$, then by definition, 
		\begin{align*}
			\beta^{(1)} &= \beta^{(0)}+ \eta  A^{T} (b-A \beta^{(0)}) \\
			&= \eta v - \eta M \beta^{(0)} \\
			&= \eta v 
		\end{align*}
		Assuming the statement $\beta^{(N)} = \eta \sum_{k=0}^{N-1} (I-\eta M)^k v$ is true for $N=n$, then for $N=n+1$ we have
		\begin{align*}
			\beta^{(n+1)} &= (I-\eta M) \beta^{(n)} + \eta v \tag{definition} \\
			&= (I-\eta M) \eta  \sum_{k=0}^{n-1} (I-\eta M)^k v + \eta v \tag{induction hypothesis} \\
			&= \eta  \sum_{k=0}^{n-1} (I-\eta M)^{(k+1)} v + \eta (I-\eta M)^0 v \\
			&= \eta  \sum_{k=1}^{n} (I-\eta M)^{(k)} v + \eta (I-\eta M)^0 v \\
			&= \eta  \sum_{k=0}^{n} (I-\eta M)^{(k)} v
		\end{align*}
	\end{proof}
\subsection*{(iii)}
	Since $M$ is a real symmetric matrix, it has a decomposition $M = P D P^{-1}$, 
	where $P$ is a matrix composed of orthogonal eigenvectors corresponding to distinct eigenvalues,
	and $D:= \text{diag}(\lambda_1,\dots, \lambda_d)$.
	\begin{align*}
		&\eta \sum_{k=0}^{N-1} (I-\eta M)^k \\
		=& \eta \sum_{k=0}^{N-1} \sum_{i=0}^{k} I^{k-i}(-\eta M)^i \\
		=&  \sum_{k=0}^{N-1} \sum_{i=0}^{k} (-1)^i\eta^{i+1} M^{i} \\
		=&  \sum_{k=0}^{N-1} \sum_{i=0}^{k} (-1)^i\eta^{i+1} PD^{i}P^{-1} \\
		=& P  \left(\sum_{k=0}^{N-1} \sum_{i=0}^{k}  (-1)^i\eta^{i+1}  \text{diag}(\lambda_1^i,\dots, \lambda_d^i) \right) P^{-1} \\
		=& P \, \text{diag} \left(\sum_{k=0}^{N-1} \sum_{i=0}^{k}  (-1)^i\eta^{i+1} \lambda_1^i,\dots, 
		\sum_{k=0}^{N-1} \sum_{i=0}^{k}  (-1)^i\eta^{i+1} \lambda_d^i \right)  P^{-1}\\
		=& P \, \text{diag} \left(
			\sum_{i=0}^{N-1} (N-i) (-1)^{i}\eta^{i+1}\lambda^i_1
			\dots, 
			\sum_{i=0}^{N-1} (N-i) (-1)^{i}\eta^{i+1}\lambda^i_d
		\right)  P^{-1}
	\end{align*}
	Since $I$, $M$ are symmetric, $\eta \sum_{k=0}^{N-1} (I-\eta M)^k$ is symmetric, 
	and its eiganvalues $\lambda_1',\dots, \lambda_d'$ are given by
	\begin{align*}
		\lambda_j' = \sum_{i=0}^{N-1} (N-i) (-1)^{i}\eta^{i+1}\lambda^i_j
	\end{align*}
	for any $j=1,\dots,d$.
\subsection*{(iv)}
	\begin{proof}
	We start by deriving a iterative formula for the difference
	\begin{align*}
		& ||\beta^{(N)} - \hat{\beta}||_2^2 \\
		=& \left( (I-\eta M)^N \hat\beta \right)^T \left( (I-\eta M)^N \hat\beta \right) \\
		=& \, \hat\beta^T (I-\eta M)^{2N} \hat\beta \\
		\leq& \, \hat\beta^T (I-2N\eta M) \hat\beta \tag{Taylor's Theorem} \\
		\leq& \, || I-2N\eta M ||_2 \cdot ||\hat{\beta}||_2^2 \\
		\leq & \, (1-2N\eta \lambda_{\text{min}}) ||\hat{\beta}||_2^2 
		\tag{Since $\sigma_{\text{max}}(I-2N\eta M) = 1-2N\eta \sigma_{\text{min}}(M)$} \\
		\leq& \, e^{-2N\eta\lambda_{\text{min}}}||\hat{\beta}||_2^2 
	\end{align*}
	\end{proof}




\section*{Problem 5}
\subsection*{Part 0}
	Ignoring the sensitive attribute does not guarantee fairness because there may be 
	other attributes with correlation to that sensitive attributes. 
	For instance, assume that thhe sensitive attribute is income level, 
	but you also have attributes like number of cars owned, average spending per month, etc. 
	Simply removing income level does not prevent the model from maintaining a bias since it will still use 
	number of cars owned, average spending per month, etc. to classify the data. As a result, 
	the model will likely fail to maintain fairness across the sensitive distribution.
\subsection*{Part 1}  
	$$P[Y=1]=P_a[Y=1]$$ 
	$$P_a[Y=1]=P[Y=1|a=0]=P[Y=0|a=1]$$
	$$P[Y=1|a=0]=P[Y=0|a=1]$$
	$$P_0[Y=1]=P_1[Y=1]$$
	Generalizing:
	$$P_{a}[Y=1] = P_{a'}[Y=1] \forall a, a ' \in A$$
	$$P[y=1] = P_a[y=1] \forall a \in A$$\\








\end{document} 
