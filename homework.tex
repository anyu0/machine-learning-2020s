\documentclass[twoside,11pt]{homework}

\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{graphicx}

\coursename{COMS 4771 Machine Learning (Spring 2020)} 

\studname{Angela Wang}     
\studmail{aw3062@columbia.edu}
\hwNo{1}      
\date{02/21/2020}  
\begin{document}
\maketitle



\section*{Problem 3} 
\subsection*{(i)}
	The error rate is the probability of getting false positive or false negatives:
	\begin{align*}
		& \mathbb{P}[f_t(x)\neq y] \\
		=& \mathbb{P}[Y=y_1 | X\leq t] + \mathbb{P}[Y=y_2 | X\geq t] \\
		=& \int_{-\infty}^t  \mathbb{P}[Y=y_1 | X=x] dx + \int_{t}^\infty  \mathbb{P}[Y=y_2 | X=x] dx
	\end{align*}
\subsection*{(ii)}
	Differentiating the error rate, we have
	\begin{align*}
		& \frac{d}{dt} \mathbb{P}[f_t(x)\neq y]  \\
		=& \, \frac{d}{dt}\int_{-\infty}^t  \mathbb{P}[Y=y_1 | X=x] dx - \frac{d}{dt} \int_{\infty}^t  \mathbb{P}[Y=y_2 | X=x] dx \\
		=&\,  \mathbb{P}[Y=y_1 | X=t] - \mathbb{P}[Y=y_2 | X=t] \tag{Foundamental Theorem of Calculus}
	\end{align*}
	To minimize the error rate, we set $\frac{d}{dt} \mathbb{P}[f_t(x)\neq y] = 0$. \\
	Hence $\mathbb{P}[Y=y_1 | X=t] = \mathbb{P}[Y=y_2 | X=t]$.
\subsection*{(iii)}


\section*{Problem 4} 
\subsection*{(i)}
	\begin{proof}
		Since $M^{T} = (A^{T}A)^{T} = A^{T}A = M$, M is symmetric. 
		For any column vector $z\in \mathbb{R}^d$, $z^TMz = z^TA^TAz = (Az)^T (Az) = (Az)\cdot(Az) \geq 0$, 
		so M is positive semi-definite.
	\end{proof}
\subsection*{(ii)}
	\begin{proof}
		Base case: if $N=1$, then by definition, 
		\begin{align*}
			\beta^{(1)} &= \beta^{(0)}+ \eta  A^{T} (b-A \beta^{(0)}) \\
			&= \eta v - \eta M \beta^{(0)} \\
			&= \eta v 
		\end{align*}
		Assuming the statement $\beta^{(N)} = \eta \sum_{k=0}^{N-1} (I-\eta M)^k v$ is true for $N=n$, then for $N=n+1$ we have
		\begin{align*}
			\beta^{(n+1)} &= (I-\eta M) \beta^{(n)} + \eta v \tag{definition} \\
			&= (I-\eta M) \eta  \sum_{k=0}^{n-1} (I-\eta M)^k v + \eta v \tag{induction hypothesis} \\
			&= \eta  \sum_{k=0}^{n-1} (I-\eta M)^{(k+1)} v + \eta (I-\eta M)^0 v \\
			&= \eta  \sum_{k=1}^{n} (I-\eta M)^{(k)} v + \eta (I-\eta M)^0 v \\
			&= \eta  \sum_{k=0}^{n} (I-\eta M)^{(k)} v
		\end{align*}
	\end{proof}
\subsection*{(iii)}
	Since $M$ is a real symmetric matrix, it has a decomposition $M = P D P^{-1}$, 
	where $P$ is a matrix composed of orthogonal eigenvectors corresponding to distinct eigenvalues,
	and $D:= \text{diag}(\lambda_1,\dots, \lambda_d)$.
	\begin{align*}
		&\eta \sum_{k=0}^{N-1} (I-\eta M)^k \\
		=& \eta \sum_{k=0}^{N-1} \sum_{i=0}^{k} I^{k-i}(-\eta M)^i \\
		=&  \sum_{k=0}^{N-1} \sum_{i=0}^{k} (-1)^i\eta^{i+1} M^{i} \\
		=&  \sum_{k=0}^{N-1} \sum_{i=0}^{k} (-1)^i\eta^{i+1} PD^{i}P^{-1} \\
		=& P  \left(\sum_{k=0}^{N-1} \sum_{i=0}^{k}  (-1)^i\eta^{i+1}  \text{diag}(\lambda_1^i,\dots, \lambda_d^i) \right) P^{-1} \\
		=& P \, \text{diag} \left(\sum_{k=0}^{N-1} \sum_{i=0}^{k}  (-1)^i\eta^{i+1} \lambda_1^i,\dots, 
		\sum_{k=0}^{N-1} \sum_{i=0}^{k}  (-1)^i\eta^{i+1} \lambda_d^i \right)  P^{-1}\\
		=& P \, \text{diag} \left(
			\sum_{i=0}^{N-1} (N-i) (-1)^{i}\eta^{i+1}\lambda^i_1
			\dots, 
			\sum_{i=0}^{N-1} (N-i) (-1)^{i}\eta^{i+1}\lambda^i_d
		\right)  P^{-1}
	\end{align*}
	Since $I$, $M$ are symmetric, $\eta \sum_{k=0}^{N-1} (I-\eta M)^k$ is symmetric, 
	and its eiganvalues $\lambda_1',\dots, \lambda_d'$ are given by
	\begin{align*}
		\lambda_j' = \sum_{i=0}^{N-1} (N-i) (-1)^{i}\eta^{i+1}\lambda^i_j
	\end{align*}
	for any $j=1,\dots,d$.
\subsection*{(iv)}
	\begin{proof}
	We start by deriving a iterative formula for the difference
	\begin{align*}
		& ||\beta^{(N)} - \hat{\beta}||_2^2 \\
		=& \left( (I-\eta M)^N \hat\beta \right)^T \left( (I-\eta M)^N \hat\beta \right) \\
		=& \, \hat\beta^T (I-\eta M)^{2N} \hat\beta \\
		\leq& \, \hat\beta^T (I-2N\eta M) \hat\beta \tag{Taylor's Theorem} \\
		\leq& \, || I-2N\eta M ||_2 \cdot ||\hat{\beta}||_2^2 \\
		\leq & \, (1-2N\eta \lambda_{\text{min}}) ||\hat{\beta}||_2^2 
		\tag{Since $\sigma_{\text{max}}(I-2N\eta M) = 1-2N\eta \sigma_{\text{min}}(M)$} \\
		\leq& \, e^{-2N\eta\lambda_{\text{min}}}||\hat{\beta}||_2^2 
	\end{align*}
	\end{proof}




\section*{Problem 5}
\subsection*{Part 0}
	Ignoring the sensitive attribute does not guarantee fairness because there may be 
	other attributes with correlation to that sensitive attributes. 
	For instance, assume that thhe sensitive attribute is income level, 
	but you also have attributes like number of cars owned, average spending per month, etc. 
	Simply removing income level does not prevent the model from maintaining a bias since it will still use 
	number of cars owned, average spending per month, etc. to classify the data. As a result, 
	the model will likely fail to maintain fairness across the sensitive distribution.
\subsection*{Part 1}  
	$$P[Y=1]=P_a[Y=1]$$ 
	$$P_a[Y=1]=P[Y=1|a=0]=P[Y=0|a=1]$$
	$$P[Y=1|a=0]=P[Y=0|a=1]$$
	$$P_0[Y=1]=P_1[Y=1]$$
	Generalizing:
	$$P_{a}[Y=1] = P_{a'}[Y=1] \forall a, a ' \in A$$
	$$P[y=1] = P_a[y=1] \forall a \in A$$\\








\end{document} 
